import logging
import ollama

# Ativar o logging
logging.basicConfig(level=logging.DEBUG)

class AIModel:
    def __init__(self, model_name="mistral"):
        self.model_name = model_name
        logging.info(f"Model {model_name} is ready to use with Ollama.")

    def generate_response(self, prompt):
        try:
            logging.debug(f"Sending prompt to the model: {prompt}")

            # Gerar a resposta com Ollama
            response = ollama.chat(model=self.model_name, messages=[
                {'role': 'user', 'content': prompt}
            ])

            # Extraindo a resposta do modelo
            message = response.get('message', {}).get('content', '')

            logging.debug(f"Response generated by the model: {message}")

            # Verifica se a resposta está vazia
            if not message:
                logging.warning("The response generated by the model was empty.")
                return "Sorry, I couldn't generate a response."

            # Adiciona a informação do modelo no final da resposta
            return f"{message}\n\nAI Model: {self.model_name}"

        except Exception as e:
            logging.error(f"Unexpected error while generating response: {str(e)}")
            return f"An unexpected error occurred: {str(e)}"
